#!/usr/bin/env python3
"""
GR00T N1-2B End-to-End Main
- ì‹¤ì‹œê°„ Vision/State/Text ìˆ˜ì§‘
- ë°ì´í„° transform ë° ì¶”ë¡ 
- ì•¡ì…˜ í† í°ì„ ë¡œë´‡ì— ì‹¤ì‹œê°„ ì „ë‹¬
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

import os
import time
import logging
import numpy as np
import torch
import argparse
import json

GR00T_N1_2B_PATH = str(Path(__file__).parent.parent / "GR00T-N1-2B")
sys.path.append(GR00T_N1_2B_PATH)
sys.path.append(str(Path(__file__).parent))

from gr00t.model.policy import Gr00tPolicy
from gr00t.experiment.data_config import DATA_CONFIG_MAP
from gr00t.data.embodiment_tags import EmbodimentTag
from model.action_decoder import EEFCommand, create_action_decoder
from data.collectors.vision_collector import VisionCollectorManager
from data.collectors.text_collector import TextCollectorManager
from control.robot_controller import RobotController
from control.safety_manager import SafetyManager
from communication.hardware_bridge import PiperHardwareBridge as HardwareBridge
from config.hardware_config import get_hardware_config
from gr00t.data.schema import DatasetMetadata


def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def parse_arguments():
    parser = argparse.ArgumentParser(description="GR00T N1-2B End-to-End Main")
    parser.add_argument("--dry-run", action="store_true", help="ë¡œë´‡ì— ì‹¤ì œ ëª…ë ¹ì„ ë³´ë‚´ì§€ ì•Šê³  ì•¡ì…˜ í† í°ë§Œ ì¶œë ¥")
    parser.add_argument("--mock-vision", action="store_true", help="Vision ì…ë ¥ì„ mock ë°ì´í„°ë¡œ ëŒ€ì²´")
    return parser.parse_args()


def collect_vision(vision_manager=None, mock_vision=False):
    if mock_vision or vision_manager is None:
        obs = {
            'video.left_wrist_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
            'video.right_wrist_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
            'video.front_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
        }
        return obs
    # VisionCollectorManagerì—ì„œ í”„ë ˆì„ ìˆ˜ì§‘
    frames = vision_manager.get_all_frames()
    obs = {}
    for k, v in frames.items():
        # (224,224,3)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ ë° RGB ë³€í™˜ ë³´ì¥
        if v.shape[1] != 224 or v.shape[0] != 224:
            import cv2
            v = cv2.resize(v, (224, 224))
        if v.shape[-1] == 3 and v.dtype != np.uint8:
            v = v.astype(np.uint8)
        obs[k] = v[None, ...]  # (1,224,224,3)
    return obs


def collect_state(hardware_bridge):
    # ì‹¤ì œ í•˜ë“œì›¨ì–´ ë¸Œë¦¿ì§€ì—ì„œ ë¡œë´‡ ìƒíƒœ ìˆ˜ì§‘
    # left/right arm state dict ë³‘í•©
    left_state = hardware_bridge.get_arm_state('left')
    right_state = hardware_bridge.get_arm_state('right')
    obs = {}
    if left_state:
        obs['state.left_arm_eef_pos'] = left_state.get('left_arm_eef_pos', np.zeros(3, dtype=np.float32))
        obs['state.left_arm_eef_quat'] = left_state.get('left_arm_eef_quat', np.zeros(4, dtype=np.float32))
        obs['state.left_gripper_qpos'] = np.array([left_state.get('left_gripper_qpos', 0.0)], dtype=np.float32)
    else:
        obs['state.left_arm_eef_pos'] = np.zeros(3, dtype=np.float32)
        obs['state.left_arm_eef_quat'] = np.zeros(4, dtype=np.float32)
        obs['state.left_gripper_qpos'] = np.zeros(1, dtype=np.float32)
    if right_state:
        obs['state.right_arm_eef_pos'] = right_state.get('right_arm_eef_pos', np.zeros(3, dtype=np.float32))
        obs['state.right_arm_eef_quat'] = right_state.get('right_arm_eef_quat', np.zeros(4, dtype=np.float32))
        obs['state.right_gripper_qpos'] = np.array([right_state.get('right_gripper_qpos', 0.0)], dtype=np.float32)
    else:
        obs['state.right_arm_eef_pos'] = np.zeros(3, dtype=np.float32)
        obs['state.right_arm_eef_quat'] = np.zeros(4, dtype=np.float32)
        obs['state.right_gripper_qpos'] = np.zeros(1, dtype=np.float32)
    return obs


def collect_text(text_collector):
    # ì‹¤ì œ í„°ë¯¸ë„ì—ì„œ ëª…ë ¹ ì…ë ¥
    # return text_collector.get_text()
    return 'test'  # ë”ë¯¸ ì…ë ¥


def transform_data(raw_obs, modality_transform):
    # state vector í•©ì¹˜ê¸° (metadata.json ìˆœì„œ)
    state_vec = np.concatenate([
        raw_obs['state.right_arm_eef_pos'],
        raw_obs['state.right_arm_eef_quat'],
        raw_obs['state.right_gripper_qpos'],
        raw_obs['state.left_arm_eef_pos'],
        raw_obs['state.left_arm_eef_quat'],
        raw_obs['state.left_gripper_qpos'],
    ], axis=0)
    obs = dict(raw_obs)  # shallow copy
    obs['state'] = state_vec[None, :]  # (1, 16)
    # state.* keyëŠ” ëª¨ë‘ ì‚­ì œ (stateë§Œ ë‚¨ê¹€)
    for k in list(obs.keys()):
        if k.startswith('state.') and k != 'state':
            del obs[k]
    print("[DEBUG] obs keys and shapes before transform:")
    for k, v in obs.items():
        if hasattr(v, 'shape'):
            print(f"  {k}: shape={v.shape}, dtype={getattr(v, 'dtype', type(v))}")
        else:
            print(f"  {k}: {type(v)}")
    return modality_transform(obs)


def run_inference(policy, transformed_obs):
    with torch.no_grad():
        policy.model.eval()
        device_obs = {k: (v.cpu() if isinstance(v, torch.Tensor) else v) for k, v in transformed_obs.items()}
        return policy.get_action(device_obs)


def execute_action(hardware_bridge, action_token, dry_run=False):
    # action_token: dict, ì˜ˆì‹œ: {'left': {...}, 'right': {...}}
    for arm_name in ['left', 'right']:
        cmd_dict = action_token.get(arm_name)
        if cmd_dict is None:
            continue
        # EEFCommandë¡œ ë³€í™˜ (trajectory_executor.py ì°¸ê³ )
        try:
            eef_cmd = EEFCommand(
                timestamp=cmd_dict.get('timestamp', time.time()),
                position=np.array(cmd_dict['position'], dtype=np.float32),
                rotation=np.array(cmd_dict['rotation'], dtype=np.float32),
                gripper=cmd_dict.get('gripper', 0.5)
            )
        except Exception as e:
            print(f"[ERROR] Invalid action_token for {arm_name}: {e}")
            continue
        if dry_run:
            print(f"[DRY-RUN] {arm_name} EEFCommand: {eef_cmd}")
        else:
            try:
                hardware_bridge.send_arm_command(arm_name, eef_cmd)
            except Exception as e:
                print(f"[ERROR] Failed to send {arm_name} command: {e}")


def main():
    args = parse_arguments()
    setup_logging()
    logger = logging.getLogger("GR00T-N1-2B-Main")
    logger.info(f"ğŸš€ GR00T N1-2B End-to-End Main ì‹œì‘ (dry-run={args.dry_run}, mock-vision={args.mock_vision})")

    # 1. í•˜ë“œì›¨ì–´/ë¡œë´‡/í…ìŠ¤íŠ¸ collector ì´ˆê¸°í™”
    hardware_bridge = HardwareBridge()
    robot_controller = RobotController(hardware_bridge)
    hw_config = get_hardware_config()
    safety_manager = SafetyManager(hw_config)

    # í…ìŠ¤íŠ¸ ì…ë ¥ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    text_collector = TextCollectorManager()
    text_collector.start_collection()

    # VisionCollectorManager ì¤€ë¹„
    vision_manager = None
    if not args.mock_vision:
        vision_manager = VisionCollectorManager(use_mock=False)
        vision_manager.start_all_cameras()

    # 2. ëª¨ë¸/transform ë¡œë“œ
    embodiment_name = "dual_piper_arm"
    data_config = DATA_CONFIG_MAP[embodiment_name]
    modality_config = data_config.modality_config()
    modality_transform = data_config.transform()

    # metadata.json ë¡œë“œ ë° set_metadata ì ìš©
    metadata_path = os.path.join("/home/rosota/GR00T-N1-2B/experiment_cfg", "metadata.json")
    with open(metadata_path, "r") as f:
        metadatas = json.load(f)
    meta_dict = metadatas["embodiment_tags"][embodiment_name]
    metadata = DatasetMetadata.model_validate(meta_dict)
    modality_transform.set_metadata(metadata)

    policy = Gr00tPolicy(
        model_path=GR00T_N1_2B_PATH,
        embodiment_tag=embodiment_name,
        modality_config=modality_config,
        modality_transform=modality_transform,
        denoising_steps=None,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    logger.info("ëª¨ë¸ ë° transform ë¡œë“œ ì™„ë£Œ")

    # 3. ì•¡ì…˜ ë””ì½”ë” ì¤€ë¹„
    action_decoder = create_action_decoder(embodiment_name)

    print("\n==== GR00T N1-2B End-to-End Main ====")
    print(f"í„°ë¯¸ë„ì—ì„œ ìì—°ì–´ ëª…ë ¹ì„ ì…ë ¥í•˜ì„¸ìš”. (quit/exit: ì¢…ë£Œ) [dry-run={args.dry_run}, mock-vision={args.mock_vision}]")

    # 4. ë©”ì¸ ë£¨í”„
    try:
        while True:
            # 1. ë°ì´í„° ìˆ˜ì§‘
            vision_obs = collect_vision(vision_manager=vision_manager, mock_vision=args.mock_vision)
            state_obs = collect_state(hardware_bridge)
            # ì‹¤ì œ í…ìŠ¤íŠ¸ ì…ë ¥ ë°›ê¸° (ëª…ë ¹ì´ ë“¤ì–´ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°)
            text_input = None
            while not text_input:
                text_data = text_collector.get_latest_command()
                text_input = text_data.get('annotation.language.instruction', None)
                if not text_input:
                    time.sleep(0.1)
            if text_input.lower() in ["quit", "exit"]:
                print("[ì¢…ë£Œ]")
                break

            # 2. í†µí•© observation ìƒì„±
            raw_obs = {**vision_obs, **state_obs, 'annotation.language.instruction': text_input}

            # 3. transform
            try:
                transformed_obs = transform_data(raw_obs, modality_transform)
            except Exception as e:
                logger.error(f"[Transform ì˜¤ë¥˜] {e}")
                import traceback
                traceback.print_exc()
                continue

            # 4. ì¶”ë¡ 
            try:
                action_token = run_inference(policy, transformed_obs)
            except Exception as e:
                logger.error(f"[ì¶”ë¡  ì˜¤ë¥˜] {e}")
                import traceback
                traceback.print_exc()
                continue

            # 5. ì•¡ì…˜ í† í° â†’ trajectory ë³€í™˜ (smoothing/blending í¬í•¨)
            try:
                trajectory = action_decoder.decode_action(action_token)
                if not trajectory:
                    logger.warning("ë””ì½”ë”©ëœ trajectoryê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
            except Exception as e:
                logger.error(f"[ì•¡ì…˜ ë””ì½”ë”© ì˜¤ë¥˜] {e}")
                import traceback
                traceback.print_exc()
                continue

            # 6. trajectory ì‹¤í–‰ (ê° stepì„ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë´‡ì— ì „ë‹¬)
            for step in trajectory:
                for arm_name in ['left', 'right']:
                    eef_cmd = step.get(arm_name)
                    if eef_cmd is None:
                        continue
                    if args.dry_run:
                        print(f"[DRY-RUN] {arm_name} EEFCommand: {eef_cmd}")
                    else:
                        try:
                            hardware_bridge.send_arm_command(arm_name, eef_cmd)
                        except Exception as e:
                            print(f"[ERROR] Failed to send {arm_name} command: {e}")
                time.sleep(0.1)  # trajectory_executorì˜ dtì™€ ë§ì¶”ê¸°

            # ì¶”ë¡  ì‚¬ì´ì— 0.5ì´ˆ ëŒ€ê¸°
            import time
            time.sleep(0.5)

            # 7. ë¡œê¹…/ëª¨ë‹ˆí„°ë§
            logger.info(f"Trajectory executed: {len(trajectory)} steps")

    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ì ì¢…ë£Œ ìš”ì²­. ì‹œìŠ¤í…œ ì¢…ë£Œ.")
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if vision_manager is not None:
            vision_manager.stop_all_cameras()
        if text_collector is not None:
            text_collector.stop_collection()

if __name__ == "__main__":
    main() 