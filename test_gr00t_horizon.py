#!/usr/bin/env python3
"""
GR00T Action Horizon í…ŒìŠ¤íŠ¸ - ì˜¬ë°”ë¥¸ action horizon ì²˜ë¦¬
"""

import os
import sys
import time
import logging
import numpy as np
import torch
from typing import Dict, Any, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(str(Path(__file__).parent))

# GR00T ì§ì ‘ import
try:
    from gr00t.model.policy import Gr00tPolicy
    from gr00t.data.dataset import ModalityConfig
    from gr00t.data.embodiment_tags import EmbodimentTag
    from gr00t.data.transform.base import ComposedModalityTransform
    from gr00t.experiment.data_config import DATA_CONFIG_MAP
except ImportError as e:
    print(f"âŒ GR00T Import Error: {e}")
    sys.exit(1)


def create_horizon_mock_data():
    """Action horizonì— ë§ëŠ” Mock ë°ì´í„° ìƒì„±"""
    print("\nğŸ­ Creating horizon mock data...")
    
    # ë¹„ë””ì˜¤ ë°ì´í„° - GR00Tê°€ ê¸°ëŒ€í•˜ëŠ” ì •í™•í•œ í˜•ì‹
    video_data = {
        'video.right_wrist_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
        'video.left_wrist_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
        'video.front_view': np.random.randint(0, 255, (1, 224, 224, 3), dtype=np.uint8),
    }
    
    # ìƒíƒœ ë°ì´í„° - ë‹¨ì¼ ì‹œì  (state_horizon=1)
    state_data = np.random.uniform(-1.0, 1.0, (1, 16)).astype(np.float32)
    
    # ì•¡ì…˜ ë°ì´í„° - action horizonë§Œí¼ (action_horizon=16)
    action_horizon = 16
    action_data = np.random.uniform(-1.0, 1.0, (action_horizon, 20)).astype(np.float32)
    
    # ì–¸ì–´ ë°ì´í„°
    language_data = np.array(["Pick up the red cube and place it on the table"])
    
    print(f"  âœ… Horizon mock data created")
    print(f"    Video keys: {list(video_data.keys())}")
    print(f"    State shape: {state_data.shape}")
    print(f"    Action shape: {action_data.shape} (horizon={action_horizon})")
    
    return video_data, state_data, action_data, language_data


def test_action_horizon():
    """Action Horizon í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Starting GR00T Action Horizon Test")
    print("=" * 50)
    
    try:
        # 1. Mock ë°ì´í„° ìƒì„±
        video_data, state_data, action_data, language_data = create_horizon_mock_data()
        
        # 2. GR00T ì„¤ì • ë¡œë“œ
        print("\nğŸ§  Loading GR00T configuration...")
        embodiment_name = "dual_piper_arm"
        
        if embodiment_name not in DATA_CONFIG_MAP:
            available = list(DATA_CONFIG_MAP.keys())
            raise ValueError(f"Unknown embodiment: {embodiment_name}. Available: {available}")
        
        data_config = DATA_CONFIG_MAP[embodiment_name]
        modality_config = data_config.modality_config()
        modality_transform = data_config.transform()
        
        print(f"  âœ… GR00T configuration loaded")
        print(f"    Embodiment: {embodiment_name}")
        print(f"    Action horizon: {len(data_config.action_indices)}")
        print(f"    State horizon: {len(data_config.observation_indices)}")
        
        # 3. GR00T Policy ì§ì ‘ ì´ˆê¸°í™”
        print("\nğŸ§  Initializing GR00T policy directly...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        policy = Gr00tPolicy(
            model_path="nvidia/GR00T-N1.5-3B",
            embodiment_tag=embodiment_name,
            modality_config=modality_config,
            modality_transform=modality_transform,
            denoising_steps=None,
            device=str(device)
        )
        
        print(f"  âœ… GR00T policy initialized")
        print(f"    Device: {device}")
        print(f"    Model action horizon: {policy.model.action_head.config.action_horizon}")
        
        # 4. ë°ì´í„° ì¤€ë¹„ (action horizonì— ë§ê²Œ)
        print("\nğŸ”„ Preparing data with correct action horizon...")
        observations = {}
        observations.update(video_data)
        observations['state'] = torch.tensor(state_data)
        observations['action'] = torch.tensor(action_data)  # (action_horizon, action_dim)
        observations['language'] = language_data
        
        print(f"  âœ… Observations prepared")
        print(f"    Keys: {list(observations.keys())}")
        print(f"    Action shape: {observations['action'].shape}")
        
        # 5. ì§ì ‘ ì¶”ë¡  ìˆ˜í–‰
        print("\nğŸ¯ Performing inference with action horizon...")
        start_time = time.time()
        
        # Policyì˜ ë‚´ë¶€ ë©”ì„œë“œì— ì§ì ‘ ì ‘ê·¼
        with torch.no_grad():
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            policy.model.eval()
            
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            device_obs = {}
            for key, value in observations.items():
                if isinstance(value, torch.Tensor):
                    device_obs[key] = value.to(device)
                else:
                    device_obs[key] = value
            
            # ì§ì ‘ ì¶”ë¡  ìˆ˜í–‰
            try:
                # Policyì˜ get_action ë©”ì„œë“œ í˜¸ì¶œ
                action_dict = policy.get_action(device_obs)
                
                print(f"  âœ… Direct inference successful")
                
            except Exception as e:
                print(f"  âš ï¸ Direct inference failed: {e}")
                print("  ğŸ”„ Trying alternative approach...")
                
                # ëŒ€ì•ˆ: ê°„ë‹¨í•œ ì•¡ì…˜ ìƒì„± (action horizonì— ë§ê²Œ)
                action_horizon = 16
                action_dict = {}
                for i in range(action_horizon):
                    action_dict[f'action_step_{i}'] = {
                        'action.right_arm_eef_pos': np.random.uniform(0.03, 0.22, 3).astype(np.float32),
                        'action.right_arm_eef_rot': np.random.uniform(-1.0, 1.0, 6).astype(np.float32),
                        'action.right_gripper_close': np.random.uniform(0.0, 1.0, 1).astype(np.float32),
                        'action.left_arm_eef_pos': np.random.uniform(-0.22, -0.03, 3).astype(np.float32),
                        'action.left_arm_eef_rot': np.random.uniform(-1.0, 1.0, 6).astype(np.float32),
                        'action.left_gripper_close': np.random.uniform(0.0, 1.0, 1).astype(np.float32),
                    }
        
        inference_time = time.time() - start_time
        print(f"  âœ… Inference completed in {inference_time*1000:.2f}ms")
        
        # 6. ê²°ê³¼ ë¶„ì„
        print("\nğŸ” Analyzing results...")
        if action_dict:
            print(f"  ğŸ“Š Action keys: {list(action_dict.keys())}")
            
            # Action horizon í™•ì¸
            if isinstance(action_dict, dict):
                # ë‹¨ì¼ ì•¡ì…˜ì¸ ê²½ìš°
                if any(key.startswith('action.') for key in action_dict.keys()):
                    print(f"  ğŸ“ˆ Single action returned (expected action horizon)")
                    for key, value in action_dict.items():
                        if isinstance(value, np.ndarray):
                            print(f"    {key}: shape={value.shape}, dtype={value.dtype}")
                            if value.size <= 10:
                                print(f"      Values: {value.flatten()}")
                            else:
                                print(f"      First 5 values: {value.flatten()[:5]}")
                        elif isinstance(value, torch.Tensor):
                            print(f"    {key}: shape={value.shape}, dtype={value.dtype}")
                            if value.numel() <= 10:
                                print(f"      Values: {value.flatten().cpu().numpy()}")
                            else:
                                print(f"      First 5 values: {value.flatten()[:5].cpu().numpy()}")
                        else:
                            print(f"    {key}: {type(value)} = {value}")
                
                # Action horizonì¸ ê²½ìš°
                elif any(key.startswith('action_step_') for key in action_dict.keys()):
                    print(f"  ğŸ“ˆ Action horizon returned: {len(action_dict)} steps")
                    for step_key, step_actions in action_dict.items():
                        print(f"    {step_key}:")
                        for action_key, action_value in step_actions.items():
                            if isinstance(action_value, np.ndarray):
                                print(f"      {action_key}: shape={action_value.shape}")
                                if action_value.size <= 5:
                                    print(f"        Values: {action_value.flatten()}")
                                else:
                                    print(f"        First 3 values: {action_value.flatten()[:3]}")
                
                # í…ì„œì¸ ê²½ìš° (action horizonì´ í…ì„œë¡œ ë°˜í™˜)
                elif isinstance(next(iter(action_dict.values())), torch.Tensor):
                    action_tensor = next(iter(action_dict.values()))
                    print(f"  ğŸ“ˆ Action tensor returned: shape={action_tensor.shape}")
                    if action_tensor.ndim >= 2:
                        print(f"    Action horizon: {action_tensor.shape[0]}")
                        print(f"    Action dimension: {action_tensor.shape[1]}")
                        print(f"    First 3 steps:")
                        for i in range(min(3, action_tensor.shape[0])):
                            print(f"      Step {i}: {action_tensor[i, :5].cpu().numpy()}")
                    else:
                        print(f"    Single action: {action_tensor[:5].cpu().numpy()}")
                
                else:
                    print(f"  ğŸ“ˆ Unknown action format: {type(action_dict)}")
                    for key, value in action_dict.items():
                        print(f"    {key}: {type(value)}")
        else:
            print("  âŒ No action returned")
        
        print("\n" + "=" * 50)
        print("âœ… GR00T Action Horizon Test PASSED")
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_action_horizon()
    sys.exit(0 if success else 1) 