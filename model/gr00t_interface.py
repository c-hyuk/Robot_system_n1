"""
GR00T ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
ê¸°ì¡´ GR00T Policyë¥¼ ìš°ë¦¬ ì‹œìŠ¤í…œì— ë§ê²Œ ë˜í•‘
"""

import time
from typing import Dict, Any, Optional, Union
from pathlib import Path
import logging
import torch
import numpy as np

# GR00T ê¸°ë³¸ import
from gr00t.model.policy import Gr00tPolicy, BasePolicy
from gr00t.data.dataset import ModalityConfig
from gr00t.data.embodiment_tags import EmbodimentTag
from gr00t.data.transform.base import ComposedModalityTransform
from gr00t.experiment.data_config import DATA_CONFIG_MAP

# ìš°ë¦¬ ì‹œìŠ¤í…œ import
from utils.data_types import RobotData
from data.integrated_pipeline import IntegratedDataPipeline


class DualPiperGR00TInterface:
    """Dual Piper ë¡œë´‡ìš© GR00T ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(
        self,
        model_path: str,
        embodiment_name: str = "dual_piper_arm",
        denoising_steps: Optional[int] = None,
        device: Union[int, str] = "cuda" if torch.cuda.is_available() else "cpu",
        use_mock_data: bool = False
    ):
        """
        GR00T ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_path: GR00T ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë˜ëŠ” HuggingFace Hub ID
            embodiment_name: ë¡œë´‡ embodiment ì´ë¦„ (DATA_CONFIG_MAPì˜ í‚¤)
            denoising_steps: Flow matching ë””ë…¸ì´ì§• ìŠ¤í… ìˆ˜
            device: ì‹¤í–‰ ì¥ì¹˜
            use_mock_data: Mock ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
        """
        self.model_path = model_path
        self.embodiment_name = embodiment_name
        self.device = device
        self.use_mock_data = use_mock_data
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger("DualPiperGR00TInterface")
        
        # ì„¤ì • ë° Transform ë¡œë“œ
        self._load_config_and_transforms()
        
        # GR00T Policy ì´ˆê¸°í™”
        self._initialize_gr00t_policy(denoising_steps)
        
        # ë°ì´í„° íŒŒì´í”„ë¼ì¸ (ì„ íƒì )
        self.data_pipeline: Optional[IntegratedDataPipeline] = None
        
        # ì„±ëŠ¥ í†µê³„
        self.total_inferences = 0
        self.total_inference_time = 0.0
        self.start_time = time.time()
        
        self.logger.info(f"GR00T interface initialized for {embodiment_name}")
    
    def _load_config_and_transforms(self):
        """ì„¤ì • ë° Transform ë¡œë“œ"""
        if self.embodiment_name not in DATA_CONFIG_MAP:
            available = list(DATA_CONFIG_MAP.keys())
            raise ValueError(f"Unknown embodiment: {self.embodiment_name}. Available: {available}")
        
        # GR00T ë°ì´í„° ì„¤ì • ë¡œë“œ
        self.data_config = DATA_CONFIG_MAP[self.embodiment_name]
        self.modality_config = self.data_config.modality_config()
        self.modality_transform = self.data_config.transform()
        
        self.logger.info(f"Loaded config for embodiment: {self.embodiment_name}")
    
    def _initialize_gr00t_policy(self, denoising_steps: Optional[int]):
        """GR00T Policy ì´ˆê¸°í™”"""
        try:
            # Embodiment tag ì„¤ì •
            if self.embodiment_name in ["dual_piper_arm"]:
                embodiment_tag = "bimanual_arm"  # GR00Tì˜ ê¸°ì¡´ íƒœê·¸ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ ê²ƒ
            else:
                embodiment_tag = self.embodiment_name
            
            # GR00T Policy ìƒì„±
            self.policy = Gr00tPolicy(
                model_path=self.model_path,
                embodiment_tag=embodiment_tag,
                modality_config=self.modality_config,
                modality_transform=self.modality_transform,
                denoising_steps=denoising_steps,
                device=self.device
            )
            
            self.logger.info("GR00T policy initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize GR00T policy: {e}")
            raise
    
    def start_data_pipeline(self) -> bool:
        """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        if self.data_pipeline is not None:
            self.logger.warning("Data pipeline already started")
            return True
        
        try:
            self.data_pipeline = IntegratedDataPipeline(
                embodiment_name=self.embodiment_name,
                use_mock=self.use_mock_data
            )
            
            success = self.data_pipeline.start()
            if success:
                self.logger.info("Data pipeline started")
            else:
                self.logger.error("Failed to start data pipeline")
            
            return success
            
        except Exception as e:
            self.logger.error(f"Error starting data pipeline: {e}")
            return False
    
    def stop_data_pipeline(self):
        """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¤‘ì§€"""
        if self.data_pipeline is not None:
            self.data_pipeline.stop()
            self.data_pipeline = None
            self.logger.info("Data pipeline stopped")
    
    def get_action_from_observations(self, observations: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê´€ì°° ë°ì´í„°ë¡œë¶€í„° ì•¡ì…˜ ì˜ˆì¸¡
        
        Args:
            observations: GR00T í˜•ì‹ì˜ ê´€ì°° ë°ì´í„°
            
        Returns:
            ì˜ˆì¸¡ëœ ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            # GR00T Policyë¡œ ì•¡ì…˜ ì˜ˆì¸¡
            action_dict = self.policy.get_action(observations)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            inference_time = time.time() - start_time
            self.total_inference_time += inference_time
            self.total_inferences += 1
            
            # ì•¡ì…˜ í›„ì²˜ë¦¬ (í•„ìš”ì‹œ)
            processed_action = self._postprocess_action(action_dict)
            
            return processed_action
            
        except Exception as e:
            self.logger.error(f"Action prediction failed: {e}")
            return self._get_safe_action()
    
    def get_action_from_pipeline(self) -> Optional[Dict[str, Any]]:
        """
        ë°ì´í„° íŒŒì´í”„ë¼ì¸ìœ¼ë¡œë¶€í„° ì•¡ì…˜ ì˜ˆì¸¡
        
        Returns:
            ì˜ˆì¸¡ëœ ì•¡ì…˜ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        if self.data_pipeline is None:
            self.logger.error("Data pipeline not started")
            return None
        
        # íŒŒì´í”„ë¼ì¸ì—ì„œ GR00T ì…ë ¥ ë°ì´í„° ìˆ˜ì§‘
        gr00t_input = self.data_pipeline.get_gr00t_input()
        if gr00t_input is None:
            self.logger.warning("No data from pipeline")
            return None
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        return self.get_action_from_observations(gr00t_input)
    
    def _postprocess_action(self, action_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ì•¡ì…˜ í›„ì²˜ë¦¬"""
        processed = {}
        
        for key, value in action_dict.items():
            if isinstance(value, torch.Tensor):
                # Tensorë¥¼ numpyë¡œ ë³€í™˜
                processed[key] = value.detach().cpu().numpy()
            else:
                processed[key] = value
        
        return processed
    
    def _get_safe_action(self) -> Dict[str, Any]:
        """ì•ˆì „í•œ ê¸°ë³¸ ì•¡ì…˜ (ì—ëŸ¬ ì‹œ ì‚¬ìš©)"""
        # ëª¨ë“  ê´€ì ˆê³¼ ì—”ë“œì´í™í„°ë¥¼ í˜„ì¬ ìœ„ì¹˜ë¡œ ìœ ì§€
        safe_action = {}
        
        for action_key in self.data_config.action_keys:
            if "joint_position" in action_key:
                # 7 DOF ê´€ì ˆ
                safe_action[action_key] = np.zeros(7, dtype=np.float32)
            elif "effector_position" in action_key:
                # 6 DOF ì—”ë“œì´í™í„° (ìœ„ì¹˜ + ìì„¸)
                safe_action[action_key] = np.zeros(6, dtype=np.float32)
        
        return safe_action
    
    def set_training_mode(self, training: bool = True):
        """í›ˆë ¨/í‰ê°€ ëª¨ë“œ ì„¤ì •"""
        if hasattr(self.policy, 'model'):
            if training:
                self.policy.model.train()
            else:
                self.policy.model.eval()
        
        # Transformë„ ëª¨ë“œ ì„¤ì •
        if training:
            self.modality_transform.train()
        else:
            self.modality_transform.eval()
        
        self.logger.info(f"Set to {'training' if training else 'evaluation'} mode")
    
    def get_modality_config(self) -> Dict[str, ModalityConfig]:
        """ëª¨ë‹¬ë¦¬í‹° ì„¤ì • ë°˜í™˜"""
        return self.policy.get_modality_config()
    
    def get_performance_stats(self) -> Dict[str, float]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        elapsed = time.time() - self.start_time
        avg_inference_time = (self.total_inference_time / self.total_inferences 
                             if self.total_inferences > 0 else 0)
        inferences_per_second = self.total_inferences / elapsed if elapsed > 0 else 0
        
        return {
            'total_inferences': self.total_inferences,
            'avg_inference_time_ms': avg_inference_time * 1000,
            'inferences_per_second': inferences_per_second,
            'uptime_seconds': elapsed
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        info = {
            'model_path': str(self.model_path),
            'embodiment_name': self.embodiment_name,
            'device': str(self.device),
            'denoising_steps': getattr(self.policy, 'denoising_steps', None)
        }
        
        if hasattr(self.policy, 'model'):
            model = self.policy.model
            info.update({
                'model_type': type(model).__name__,
                'action_horizon': getattr(model, 'action_horizon', None),
                'state_horizon': getattr(model, 'state_horizon', None)
            })
        
        return info
    
    def validate_observations(self, observations: Dict[str, Any]) -> bool:
        """ê´€ì°° ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        try:
            # í•„ìˆ˜ í‚¤ í™•ì¸
            expected_keys = set()
            for modality_key in self.modality_config.keys():
                if modality_key in observations:
                    expected_keys.add(modality_key)
            
            if len(expected_keys) == 0:
                self.logger.error("No valid modality keys found in observations")
                return False
            
            # ë°ì´í„° íƒ€ì… ë° í¬ê¸° í™•ì¸
            for key, value in observations.items():
                if not isinstance(value, (np.ndarray, torch.Tensor)):
                    self.logger.error(f"Invalid data type for {key}: {type(value)}")
                    return False
                
                if hasattr(value, 'shape') and len(value.shape) == 0:
                    self.logger.error(f"Scalar value not allowed for {key}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Validation error: {e}")
            return False
    
    def __enter__(self):
        """Context manager ì§„ì…"""
        self.start_data_pipeline()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        self.stop_data_pipeline()


# í¸ì˜ìš© í•¨ìˆ˜ë“¤
def create_dual_piper_interface(
    model_path: str,
    denoising_steps: Optional[int] = None,
    device: str = "cuda",
    use_mock_data: bool = False
) -> DualPiperGR00TInterface:
    """Dual Piper GR00T ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    return DualPiperGR00TInterface(
        model_path=model_path,
        embodiment_name="dual_piper_arm",
        denoising_steps=denoising_steps,
        device=device,
        use_mock_data=use_mock_data
    )


def test_gr00t_interface():
    """GR00T ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("Testing GR00T interface...")
    
    # Mock ëª¨ë¸ ê²½ë¡œ (ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹œ ì‹¤ì œ ê²½ë¡œ ì‚¬ìš©)
    model_path = "nvidia/gr00t-1.5b"  # ì˜ˆì‹œ ê²½ë¡œ
    
    try:
        # ì¸í„°í˜ì´ìŠ¤ ìƒì„± (Mock ëª¨ë“œ)
        interface = DualPiperGR00TInterface(
            model_path=model_path,
            embodiment_name="dual_piper_arm",  # ì‹¤ì œë¡œëŠ” ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            use_mock_data=True
        )
        
        print("âœ… Interface created successfully")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model_info = interface.get_model_info()
        print(f"Model info: {model_info}")
        
        # ëª¨ë‹¬ë¦¬í‹° ì„¤ì • í™•ì¸
        modality_config = interface.get_modality_config()
        print(f"Modality config keys: {list(modality_config.keys())}")
        
        # Mock ê´€ì°° ë°ì´í„° ìƒì„±
        mock_observations = {
            'video': np.random.randint(0, 255, (1, 3, 224, 224), dtype=np.uint8),
            'state': np.random.uniform(-1, 1, (1, 64), dtype=np.float32),
            'language': "Pick up the red cube"
        }
        
        print("\nğŸ” Testing action prediction...")
        
        # ë°ì´í„° ê²€ì¦
        is_valid = interface.validate_observations(mock_observations)
        print(f"Observations valid: {is_valid}")
        
        if is_valid:
            # ì•¡ì…˜ ì˜ˆì¸¡
            action = interface.get_action_from_observations(mock_observations)
            print(f"Predicted action keys: {list(action.keys())}")
            
            for key, value in action.items():
                if hasattr(value, 'shape'):
                    print(f"  {key}: {value.shape}")
        
        # ì„±ëŠ¥ í†µê³„
        stats = interface.get_performance_stats()
        print(f"\nPerformance stats: {stats}")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        print("Note: This is expected if the actual model path doesn't exist")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_gr00t_interface()