"""
GR00T ì¶”ë¡  ì—”ì§„
ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ì¶”ë¡  ì‹œìŠ¤í…œ
"""

import time
import threading
import queue
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass
from enum import Enum
import logging
import numpy as np
import torch

from model.gr00t_interface import DualPiperGR00TInterface
from data.integrated_pipeline import IntegratedDataPipeline


class InferenceState(Enum):
    """ì¶”ë¡  ì—”ì§„ ìƒíƒœ"""
    IDLE = "idle"
    RUNNING = "running"
    PAUSED = "paused"
    ERROR = "error"


@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì—”ì§„ ì„¤ì •"""
    target_frequency: float = 10.0  # Hz (10Hz = 100ms ê°„ê²©)
    max_queue_size: int = 5
    timeout_seconds: float = 1.0
    enable_action_smoothing: bool = True
    smoothing_alpha: float = 0.7
    enable_safety_checks: bool = True
    max_consecutive_failures: int = 3


class RealTimeInferenceEngine:
    """ì‹¤ì‹œê°„ GR00T ì¶”ë¡  ì—”ì§„"""
    
    def __init__(
        self,
        gr00t_interface: DualPiperGR00TInterface,
        config: Optional[InferenceConfig] = None
    ):
        """
        ì‹¤ì‹œê°„ ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            gr00t_interface: GR00T ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
            config: ì¶”ë¡  ì—”ì§„ ì„¤ì •
        """
        self.gr00t_interface = gr00t_interface
        self.config = config or InferenceConfig()
        
        # ìƒíƒœ ê´€ë¦¬
        self.state = InferenceState.IDLE
        self.inference_thread: Optional[threading.Thread] = None
        self.stop_event = threading.Event()
        
        # ë°ì´í„° í
        self.action_queue = queue.Queue(maxsize=self.config.max_queue_size)
        
        # ì•¡ì…˜ ìŠ¤ë¬´ë”©ìš© íˆìŠ¤í† ë¦¬
        self.action_history: List[Dict[str, np.ndarray]] = []
        self.max_history_length = 5
        
        # ì•ˆì „ì„± ê´€ë¦¬
        self.consecutive_failures = 0
        self.last_safe_action: Optional[Dict[str, np.ndarray]] = None
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.start_time = None
        self.last_inference_time = None
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.action_callbacks: List[Callable] = []
        self.error_callbacks: List[Callable] = []
        
        # ë¡œê¹…
        self.logger = logging.getLogger("RealTimeInferenceEngine")
    
    def add_action_callback(self, callback: Callable[[Dict[str, np.ndarray]], None]):
        """ì•¡ì…˜ ì¶œë ¥ ì½œë°± ì¶”ê°€"""
        self.action_callbacks.append(callback)
    
    def add_error_callback(self, callback: Callable[[Exception], None]):
        """ì—ëŸ¬ ì½œë°± ì¶”ê°€"""
        self.error_callbacks.append(callback)
    
    def start(self) -> bool:
        """ì¶”ë¡  ì—”ì§„ ì‹œì‘"""
        if self.state == InferenceState.RUNNING:
            self.logger.warning("Inference engine already running")
            return True
        
        try:
            # GR00T ì¸í„°í˜ì´ìŠ¤ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘
            if not self.gr00t_interface.start_data_pipeline():
                self.logger.error("Failed to start GR00T data pipeline")
                return False
            
            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            self.gr00t_interface.set_training_mode(False)
            
            # ì¶”ë¡  ìŠ¤ë ˆë“œ ì‹œì‘
            self.stop_event.clear()
            self.inference_thread = threading.Thread(target=self._inference_loop, daemon=True)
            self.inference_thread.start()
            
            self.state = InferenceState.RUNNING
            self.start_time = time.time()
            self.consecutive_failures = 0
            
            self.logger.info(f"Inference engine started at {self.config.target_frequency}Hz")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to start inference engine: {e}")
            self.state = InferenceState.ERROR
            self._notify_error_callbacks(e)
            return False
    
    def stop(self):
        """ì¶”ë¡  ì—”ì§„ ì¤‘ì§€"""
        if self.state == InferenceState.IDLE:
            return
        
        self.logger.info("Stopping inference engine...")
        
        # ì¤‘ì§€ ì‹ í˜¸ ë°œì†¡
        self.stop_event.set()
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.inference_thread and self.inference_thread.is_alive():
            self.inference_thread.join(timeout=2.0)
        
        # GR00T ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
        self.gr00t_interface.stop_data_pipeline()
        
        self.state = InferenceState.IDLE
        self.logger.info("Inference engine stopped")
    
    def pause(self):
        """ì¶”ë¡  ì—”ì§„ ì¼ì‹œì •ì§€"""
        if self.state == InferenceState.RUNNING:
            self.state = InferenceState.PAUSED
            self.logger.info("Inference engine paused")
    
    def resume(self):
        """ì¶”ë¡  ì—”ì§„ ì¬ê°œ"""
        if self.state == InferenceState.PAUSED:
            self.state = InferenceState.RUNNING
            self.logger.info("Inference engine resumed")
    
    def _inference_loop(self):
        """ì¶”ë¡  ë£¨í”„ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        target_interval = 1.0 / self.config.target_frequency
        
        while not self.stop_event.is_set():
            if self.state != InferenceState.RUNNING:
                time.sleep(0.1)
                continue
            
            loop_start = time.time()
            
            try:
                # ì¶”ë¡  ì‹¤í–‰
                action = self._perform_inference()
                
                if action is not None:
                    # ì•¡ì…˜ í›„ì²˜ë¦¬
                    processed_action = self._postprocess_action(action)
                    
                    # ì•ˆì „ì„± ê²€ì‚¬
                    if self._validate_action(processed_action):
                        # ì•¡ì…˜ íì— ì¶”ê°€
                        self._queue_action(processed_action)
                        
                        # ì½œë°± í˜¸ì¶œ
                        self._notify_action_callbacks(processed_action)
                        
                        # ì„±ê³µ ì¹´ìš´í„° ë¦¬ì…‹
                        self.consecutive_failures = 0
                        self.last_safe_action = processed_action.copy()
                    else:
                        self._handle_unsafe_action()
                else:
                    self._handle_inference_failure()
                
            except Exception as e:
                self.logger.error(f"Inference loop error: {e}")
                self._handle_inference_failure()
                self._notify_error_callbacks(e)
            
            # íƒ€ì´ë° ì¡°ì ˆ
            elapsed = time.time() - loop_start
            sleep_time = max(0, target_interval - elapsed)
            if sleep_time > 0:
                time.sleep(sleep_time)
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸
            self.last_inference_time = time.time()
    
    def _perform_inference(self) -> Optional[Dict[str, Any]]:
        """ì¶”ë¡  ìˆ˜í–‰"""
        inference_start = time.time()
        
        try:
            # GR00T ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì•¡ì…˜ ì˜ˆì¸¡
            action = self.gr00t_interface.get_action_from_pipeline()
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            inference_time = time.time() - inference_start
            self.total_inference_time += inference_time
            self.inference_count += 1
            
            return action
            
        except Exception as e:
            self.logger.error(f"Inference failed: {e}")
            return None
    
    def _postprocess_action(self, action: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """ì•¡ì…˜ í›„ì²˜ë¦¬"""
        processed = {}
        
        # íƒ€ì… ë³€í™˜ ë° ì •ë¦¬
        for key, value in action.items():
            if isinstance(value, torch.Tensor):
                processed[key] = value.detach().cpu().numpy()
            elif isinstance(value, np.ndarray):
                processed[key] = value.copy()
            else:
                # ë‹¤ë¥¸ íƒ€ì…ì€ numpy ë°°ì—´ë¡œ ë³€í™˜ ì‹œë„
                try:
                    processed[key] = np.array(value, dtype=np.float32)
                except:
                    self.logger.warning(f"Could not convert {key} to numpy array")
                    continue
        
        # ì•¡ì…˜ ìŠ¤ë¬´ë”© ì ìš©
        if self.config.enable_action_smoothing:
            processed = self._apply_action_smoothing(processed)
        
        return processed
    
    def _apply_action_smoothing(self, action: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """ì•¡ì…˜ ìŠ¤ë¬´ë”© ì ìš©"""
        if not self.action_history:
            # ì²« ë²ˆì§¸ ì•¡ì…˜ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            self.action_history.append(action.copy())
            return action
        
        smoothed = {}
        prev_action = self.action_history[-1]
        alpha = self.config.smoothing_alpha
        
        for key, value in action.items():
            if key in prev_action:
                # ì§€ìˆ˜ ì´ë™ í‰ê·  ì ìš©
                smoothed[key] = alpha * prev_action[key] + (1 - alpha) * value
            else:
                smoothed[key] = value
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.action_history.append(smoothed.copy())
        if len(self.action_history) > self.max_history_length:
            self.action_history = self.action_history[-self.max_history_length:]
        
        return smoothed
    
    def _validate_action(self, action: Dict[str, np.ndarray]) -> bool:
        """ì•¡ì…˜ ìœ íš¨ì„± ê²€ì¦"""
        if not self.config.enable_safety_checks:
            return True
        
        try:
            for key, value in action.items():
                # NaN/Inf ê²€ì‚¬
                if np.any(np.isnan(value)) or np.any(np.isinf(value)):
                    self.logger.error(f"Invalid values in action {key}: NaN or Inf")
                    return False
                
                # ê´€ì ˆ ì œí•œ ê²€ì‚¬ (ê°„ë‹¨í•œ ë²”ìœ„ ì²´í¬)
                if "joint_position" in key:
                    if np.any(np.abs(value) > 3.14):  # Â±180ë„ ì œí•œ
                        self.logger.warning(f"Joint position out of range in {key}")
                        # ê²½ê³ ë§Œ í•˜ê³  ê³„ì† ì§„í–‰ (í´ë¦¬í•‘ ì ìš©)
                        action[key] = np.clip(value, -3.14, 3.14)
                
                elif "effector_position" in key:
                    # ìœ„ì¹˜ëŠ” Â±2m, íšŒì „ì€ Â±180ë„ë¡œ ì œí•œ
                    if len(value) >= 3:
                        action[key][:3] = np.clip(value[:3], -2.0, 2.0)
                    if len(value) >= 6:
                        action[key][3:6] = np.clip(value[3:6], -3.14, 3.14)
            
            return True
            
        except Exception as e:
            self.logger.error(f"Action validation error: {e}")
            return False
    
    def _handle_unsafe_action(self):
        """ì•ˆì „í•˜ì§€ ì•Šì€ ì•¡ì…˜ ì²˜ë¦¬"""
        self.consecutive_failures += 1
        self.logger.warning(f"Unsafe action detected. Failures: {self.consecutive_failures}")
        
        if self.last_safe_action is not None:
            # ë§ˆì§€ë§‰ ì•ˆì „í•œ ì•¡ì…˜ ì‚¬ìš©
            self._queue_action(self.last_safe_action)
            self._notify_action_callbacks(self.last_safe_action)
        
        if self.consecutive_failures >= self.config.max_consecutive_failures:
            self.logger.error("Too many consecutive failures. Entering error state.")
            self.state = InferenceState.ERROR
    
    def _handle_inference_failure(self):
        """ì¶”ë¡  ì‹¤íŒ¨ ì²˜ë¦¬"""
        self.consecutive_failures += 1
        self.logger.error(f"Inference failure. Count: {self.consecutive_failures}")
        
        if self.consecutive_failures >= self.config.max_consecutive_failures:
            self.logger.error("Too many consecutive failures. Entering error state.")
            self.state = InferenceState.ERROR
    
    def _queue_action(self, action: Dict[str, np.ndarray]):
        """ì•¡ì…˜ì„ íì— ì¶”ê°€"""
        try:
            # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ ì•¡ì…˜ ì œê±°
            if self.action_queue.full():
                try:
                    self.action_queue.get_nowait()
                except queue.Empty:
                    pass
            
            self.action_queue.put_nowait(action)
            
        except queue.Full:
            self.logger.warning("Action queue is full")
    
    def _notify_action_callbacks(self, action: Dict[str, np.ndarray]):
        """ì•¡ì…˜ ì½œë°± í˜¸ì¶œ"""
        for callback in self.action_callbacks:
            try:
                callback(action)
            except Exception as e:
                self.logger.error(f"Action callback error: {e}")
    
    def _notify_error_callbacks(self, error: Exception):
        """ì—ëŸ¬ ì½œë°± í˜¸ì¶œ"""
        for callback in self.error_callbacks:
            try:
                callback(error)
            except Exception as e:
                self.logger.error(f"Error callback error: {e}")
    
    def get_latest_action(self, timeout: float = None) -> Optional[Dict[str, np.ndarray]]:
        """ìµœì‹  ì•¡ì…˜ ë°˜í™˜"""
        timeout = timeout or self.config.timeout_seconds
        
        try:
            return self.action_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def get_engine_status(self) -> Dict[str, Any]:
        """ì—”ì§„ ìƒíƒœ ë°˜í™˜"""
        elapsed = time.time() - self.start_time if self.start_time else 0
        avg_inference_time = (self.total_inference_time / self.inference_count 
                             if self.inference_count > 0 else 0)
        actual_frequency = self.inference_count / elapsed if elapsed > 0 else 0
        
        return {
            'state': self.state.value,
            'target_frequency': self.config.target_frequency,
            'actual_frequency': actual_frequency,
            'inference_count': self.inference_count,
            'avg_inference_time_ms': avg_inference_time * 1000,
            'consecutive_failures': self.consecutive_failures,
            'action_queue_size': self.action_queue.qsize(),
            'uptime_seconds': elapsed,
            'last_inference_ago': time.time() - self.last_inference_time if self.last_inference_time else None
        }
    
    def get_performance_metrics(self) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        status = self.get_engine_status()
        
        # ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ
        metrics = {
            'frequency_ratio': status['actual_frequency'] / status['target_frequency'] if status['target_frequency'] > 0 else 0,
            'avg_latency_ms': status['avg_inference_time_ms'],
            'failure_rate': self.consecutive_failures / max(self.inference_count, 1),
            'queue_utilization': status['action_queue_size'] / self.config.max_queue_size
        }
        
        return metrics
    
    def __enter__(self):
        """Context manager ì§„ì…"""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        self.stop()


def create_inference_engine(
    model_path: str,
    target_frequency: float = 10.0,
    use_mock_data: bool = False
) -> RealTimeInferenceEngine:
    """ì¶”ë¡  ì—”ì§„ ìƒì„±"""
    from model.gr00t_interface import create_dual_piper_interface
    
    # GR00T ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    gr00t_interface = create_dual_piper_interface(
        model_path=model_path,
        use_mock_data=use_mock_data
    )
    
    # ì¶”ë¡  ì—”ì§„ ì„¤ì •
    config = InferenceConfig(target_frequency=target_frequency)
    
    return RealTimeInferenceEngine(gr00t_interface, config)


def test_inference_engine():
    """ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    print("Testing real-time inference engine...")
    
    # Mock ì•¡ì…˜ ì½œë°±
    def action_callback(action):
        print(f"Received action: {list(action.keys())}")
        for key, value in action.items():
            if hasattr(value, 'shape'):
                print(f"  {key}: {value.shape}, range=[{value.min():.3f}, {value.max():.3f}]")
    
    def error_callback(error):
        print(f"Error occurred: {error}")
    
    try:
        # ì¶”ë¡  ì—”ì§„ ìƒì„± (Mock ëª¨ë“œ)
        engine = create_inference_engine(
            model_path="mock_model_path",
            target_frequency=5.0,  # 5Hzë¡œ í…ŒìŠ¤íŠ¸
            use_mock_data=True
        )
        
        # ì½œë°± ë“±ë¡
        engine.add_action_callback(action_callback)
        engine.add_error_callback(error_callback)
        
        print("âœ… Inference engine created")
        
        # ì¶”ë¡  ì—”ì§„ ì‹œì‘
        with engine:
            print("ğŸš€ Inference engine started")
            
            # 10ì´ˆ ë™ì•ˆ ì‹¤í–‰
            for i in range(10):
                time.sleep(1)
                
                # ìƒíƒœ í™•ì¸
                status = engine.get_engine_status()
                metrics = engine.get_performance_metrics()
                
                print(f"\nSecond {i+1}:")
                print(f"  State: {status['state']}")
                print(f"  Frequency: {status['actual_frequency']:.1f}Hz (target: {status['target_frequency']}Hz)")
                print(f"  Inference count: {status['inference_count']}")
                print(f"  Avg latency: {status['avg_inference_time_ms']:.1f}ms")
                print(f"  Queue size: {status['action_queue_size']}")
                print(f"  Performance ratio: {metrics['frequency_ratio']:.2f}")
                
                # ì•¡ì…˜ ìˆ˜ë™ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
                action = engine.get_latest_action(timeout=0.1)
                if action:
                    print(f"  Manual action fetch: {list(action.keys())}")
        
        print("âœ… Test completed successfully")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        print("Note: This is expected if the actual model path doesn't exist")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_inference_engine()